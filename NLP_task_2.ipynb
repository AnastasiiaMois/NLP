{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lyricsgenius in /anaconda3/envs/tensorflow_env/lib/python3.7/site-packages (1.8.2)\r\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /anaconda3/envs/tensorflow_env/lib/python3.7/site-packages (from lyricsgenius) (4.6.0)\r\n",
      "Requirement already satisfied: requests>=2.20.0 in /anaconda3/envs/tensorflow_env/lib/python3.7/site-packages (from lyricsgenius) (2.23.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/envs/tensorflow_env/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2020.4.5.1)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /anaconda3/envs/tensorflow_env/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda3/envs/tensorflow_env/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/envs/tensorflow_env/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (1.25.8)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "import pandas as pd\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by Eminem...\n",
      "\n",
      "Song 1: \"12 Days of Diss-Mas\"\n",
      "\n",
      "Reached user-specified song limit (1).\n",
      "Done. Found 1 songs.\n"
     ]
    }
   ],
   "source": [
    "genius = lyricsgenius.Genius(\"Hm2O62KAtRDaiE9AcUkCmQKivJeVn-7-x1Mj7oXDy0wOzvyAn8L4Jx3XmPjlKOmo\")\n",
    "artist = genius.search_artist(\"Eminem\", max_songs=1, sort=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Venom\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Not Afraid\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Lose Yourself\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Without Me\" by Eminem...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "song1 = genius.search_song(\"Venom\", artist.name)\n",
    "song1_lyrics = song1.lyrics\n",
    "song2 = genius.search_song(\"Not Afraid\", artist.name)\n",
    "song2_lyrics = song2.lyrics\n",
    "song3 = genius.search_song(\"Lose Yourself\", artist.name)\n",
    "song3_lyrics = song3.lyrics\n",
    "song4 = genius.search_song(\"Without Me\", artist.name)\n",
    "song4_lyrics = song4.lyrics\n",
    "lyrics = song2_lyrics+song4_lyrics+song3_lyrics+song1_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anastasiamoiseva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    # lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_inputs = tokenize_words(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '6',\n",
       " '7',\n",
       " '9',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'ú']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 10530\n",
      "Total vocab: 34\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 10430\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10430/10430 [==============================] - 629s 60ms/step - loss: 2.9797\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.97974, saving model to model_weights.hdf5\n",
      "Epoch 2/20\n",
      "10430/10430 [==============================] - 587s 56ms/step - loss: 2.9355\n",
      "\n",
      "Epoch 00002: loss improved from 2.97974 to 2.93550, saving model to model_weights.hdf5\n",
      "Epoch 3/20\n",
      "10430/10430 [==============================] - 933s 89ms/step - loss: 2.9341\n",
      "\n",
      "Epoch 00003: loss improved from 2.93550 to 2.93405, saving model to model_weights.hdf5\n",
      "Epoch 4/20\n",
      "10430/10430 [==============================] - 594s 57ms/step - loss: 2.9233\n",
      "\n",
      "Epoch 00004: loss improved from 2.93405 to 2.92327, saving model to model_weights.hdf5\n",
      "Epoch 5/20\n",
      "10430/10430 [==============================] - 592s 57ms/step - loss: 2.8847\n",
      "\n",
      "Epoch 00005: loss improved from 2.92327 to 2.88472, saving model to model_weights.hdf5\n",
      "Epoch 6/20\n",
      "10430/10430 [==============================] - 588s 56ms/step - loss: 2.7497\n",
      "\n",
      "Epoch 00006: loss improved from 2.88472 to 2.74969, saving model to model_weights.hdf5\n",
      "Epoch 7/20\n",
      "10430/10430 [==============================] - 602s 58ms/step - loss: 2.6687\n",
      "\n",
      "Epoch 00007: loss improved from 2.74969 to 2.66870, saving model to model_weights.hdf5\n",
      "Epoch 8/20\n",
      "10430/10430 [==============================] - 586s 56ms/step - loss: 2.6333\n",
      "\n",
      "Epoch 00008: loss improved from 2.66870 to 2.63332, saving model to model_weights.hdf5\n",
      "Epoch 9/20\n",
      "10430/10430 [==============================] - 589s 56ms/step - loss: 2.5957\n",
      "\n",
      "Epoch 00009: loss improved from 2.63332 to 2.59571, saving model to model_weights.hdf5\n",
      "Epoch 10/20\n",
      "10430/10430 [==============================] - 574s 55ms/step - loss: 2.5578\n",
      "\n",
      "Epoch 00010: loss improved from 2.59571 to 2.55780, saving model to model_weights.hdf5\n",
      "Epoch 11/20\n",
      "10430/10430 [==============================] - 575s 55ms/step - loss: 2.5124\n",
      "\n",
      "Epoch 00011: loss improved from 2.55780 to 2.51236, saving model to model_weights.hdf5\n",
      "Epoch 12/20\n",
      "10430/10430 [==============================] - 582s 56ms/step - loss: 2.4811\n",
      "\n",
      "Epoch 00012: loss improved from 2.51236 to 2.48115, saving model to model_weights.hdf5\n",
      "Epoch 13/20\n",
      "10430/10430 [==============================] - 606s 58ms/step - loss: 2.4473\n",
      "\n",
      "Epoch 00013: loss improved from 2.48115 to 2.44733, saving model to model_weights.hdf5\n",
      "Epoch 14/20\n",
      "10430/10430 [==============================] - 599s 57ms/step - loss: 2.4054\n",
      "\n",
      "Epoch 00014: loss improved from 2.44733 to 2.40539, saving model to model_weights.hdf5\n",
      "Epoch 15/20\n",
      "10430/10430 [==============================] - 574s 55ms/step - loss: 2.3663\n",
      "\n",
      "Epoch 00015: loss improved from 2.40539 to 2.36634, saving model to model_weights.hdf5\n",
      "Epoch 16/20\n",
      "10430/10430 [==============================] - 572s 55ms/step - loss: 2.3355\n",
      "\n",
      "Epoch 00016: loss improved from 2.36634 to 2.33547, saving model to model_weights.hdf5\n",
      "Epoch 17/20\n",
      "10430/10430 [==============================] - 564s 54ms/step - loss: 2.2962\n",
      "\n",
      "Epoch 00017: loss improved from 2.33547 to 2.29621, saving model to model_weights.hdf5\n",
      "Epoch 18/20\n",
      "10430/10430 [==============================] - 578s 55ms/step - loss: 2.2560\n",
      "\n",
      "Epoch 00018: loss improved from 2.29621 to 2.25598, saving model to model_weights.hdf5\n",
      "Epoch 19/20\n",
      "10430/10430 [==============================] - 569s 55ms/step - loss: 2.2228\n",
      "\n",
      "Epoch 00019: loss improved from 2.25598 to 2.22284, saving model to model_weights.hdf5\n",
      "Epoch 20/20\n",
      "10430/10430 [==============================] - 575s 55ms/step - loss: 2.1731\n",
      "\n",
      "Epoch 00020: loss improved from 2.22284 to 2.17314, saving model to model_weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a74ab4550>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=32, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" mp start heart quicker shock get shocked hospital doctor co operating rocking table operating hey wa \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la "
     ]
    }
   ],
   "source": [
    "#so that's what i got with 3 songs and 15 epochs\n",
    "\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it seady cooe take like litt seady cooe take like litt seady cooe take like litt seady cooe take lik"
     ]
    }
   ],
   "source": [
    "#so that's what i got with 4 songs and 20 epochs\n",
    "\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious (at least for me) observation: \n",
    "    this algorithm is capable of creating something that to some what extent resembles rap\n",
    "    \n",
    "I'm sure I would be getting much better results, but unfortunately my laptop is not capable to do more (and Google colab didn't work for me neither)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Existing algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anastasiamoiseva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = preprocess_text(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "text = sent_tokenize(processed_inputs)\n",
    "\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sent_tokenize(processed_inputs)]\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1803\n"
     ]
    }
   ],
   "source": [
    "new_lyrics = []\n",
    "[ new_lyrics.extend(el) for el in tokenized_text] \n",
    "\n",
    "print(len(new_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = list(bigrams(new_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "flat = list(flatten(pad_both_ends(sent, n=2) for sent in tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[]\n",
    "for word in flat:\n",
    "    X.append(model.score(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 3609 ngrams>\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, padded_sents)\n",
    "\n",
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['set', 'mind', 'man', 'goddamn', 'food', 'stamps', 'buy', 'diapers', 'movie', 'mekhi', 'phifer', 'life', 'childhood', 'cause', 'feels', 'empty', 'without', 'outro', 'eminem', 'two', 'cents', 'free', 'nuisance', 'sent', 'sent', 'sent', 'chorus', 'better', 'lose', 'music', 'moment', 'thinkin', 'time', 'put', 'like', 'job', 'ms', 'cheney', 'husband', 'heart', 'quicker', 'shock', 'get', 'bit', 'chorus', 'better', 'go', 'back', 'ropes', 'matter', 'many', 'fish', 'sea', 'empty', 'without', 'doubt', 'undoubtably', 'look', 'tearin', 'balcony', 'ifs', 'ands', 'buts', 'try', 'ask', 'infinite', 'last', 'relapse', 'album', 'still', 'follows', 'around', 'time', 'go', 'get', 'ready', 'cause', 'back', 'guess', 'back', 'shady', 'back', 'together', 'storm', 'whatever', 'weather', 'cold', 'warm', 'lettin', 'know', 'hit', 'em', 'w', 'get', 'one', 'shot', 'miss', 'chance', 'blow', 'opportunity', 'comes', 'along', 'mission', 'yells', 'bitch', 'visionary', 'vision', 'scary', 'could', 'come', 'choking', 'everybody', 'follow', 'get', 'ass', 'kicked', 'worse', 'married', 'game', 'like', 'daughters', 'raise', 'lift', 'whole', 'back', 'mangled', 'steel', 'mustang', 'jeep', 'wrangler', 'grill', 'front', 'smashed', 'much', 'wan', 'na', 'slow', 'ready', 'snap', 'moment', 'better', 'never', 'let', 'stop', 'debating', 'cause', 'feels', 'empty', 'without', 'said', 'looks', 'like', 'bees', 'stingers', 'drop', 'dead', 'long', 'stop', 'debating', 'cause', 'feels', 'empty', 'without', 'outro', 'anything', 'rectangular', 'shape', 'wait', 'face', 'fuckin', 'world', 'haters', 'make', 'like', 'sunblock', 'fuck', 'track', 'like', 'love', 'trailer', 'park', 'girls', 'go', 'round', 'outside', 'two', 'trailer', 'got', 'job', 'everybody', 'everybody', 'follow', 'cause', 'raisin', 'bar', 'shoot', 'moon', 'busy', 'gazin', 'stars', 'feel', 'strong', 'enough', 'fed', 'time', 'ago', 'liquid', 'tylenol', 'gelatins', 'think', 'still', 'follows', 'around', 'time', 'blaow', 'cause', 'need', 'little', 'controversy', 'cause', 'feels', 'empty', 'without', 'chorus', 'venom', 'got', 'urge', 'pull', 'dick', 'dirt', 'fuck', 'debbie', 'chorus', 'better', 'verse', '3', 'decision', 'get', 'em', 'cause', 'nobody', 'wants', 'see', 'try', 'ask', 'infinite', 'last', 'relapse', 'album', 'still', 'tryin', 'say', 'get', 'bit', 'chorus', 'eminem', 'created', 'monster', 'cause', 'music', 'moment', 'thinkin', 'time', 'go', 'capture', 'moment', 'thinkin', 'time', 'put', 'like', 'sunblock', 'fuck', 'track', 'like', 'road', 'together', 'storm', 'whatever', 'weather', 'cold', 'water', 'hoes', 'want', 'shady', 'chopped', 'liver', 'well', 'want', 'shady', 'give', 'signal', 'whole', 'liquor', 'vodka', 'jump', 'start', 'heart', 'problem', 'complicating', 'fcc', 'let', 'see', 'marshall', 'want', 'mo', 'cold', 'warm', 'lettin', 'know', 'way', 'startin', 'today', 'breakin', 'cage', 'standin', 'face', 'demons', 'mannin', 'hold', 'ground', 'enough', 'go', 'place', 'get', 'em', 'snare', 'bass', 'square', 'face', 'demons', 'mannin', 'hold', 'nose', 'dove', 'sold', 'nada', 'soap', 'opera', 'told', 'unfolds', 'suppose', 'old', 'partner', 'beat', 'strangler', 'attack', 'gon', 'na', 'able', 'tell', 'fuck', 'christmas', 'gift', 'curse', 'forget', 'earth', 'got', 'song', 'filled', 'shit', 'shit', 'seems', 'everybody', 'come', 'take', 'stand', 'take', 'hand', 'walk', 'road', 'intro', 'obie', '36', 'year', 'old', 'partner', 'beat', 'goes', 'da', 'da', 'dom', 'dah', 'dah', 'chorus', 'afraid', 'afraid', 'take', 'hand', 'come', 'back', 'guess', 'back', 'guess', 'go', 'get', 'stomped', 'obie', 'trice', 'obie', 'trice']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(400, random_seed=222))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=4):\n",
    "    \n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc put like prisoners helpless til scream piss screw hell meant kahlúa screw hell went hellmann rail thin filet fish sea empty without chorus better verse 1 eminem little'"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 29, random_seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, rap it is too strong (and rather flattering) word for what it generated after all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, i do not know which model is better\n",
    "Probably the first one (as it's actually generates something), the second one just randomnly puts words from Eminem's songs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
To entertain You a little bit a rap (poem) by my self:

sorry for my bad 
i’m stupid in my head
don’t know i how to code
hope u didn’t get bored
